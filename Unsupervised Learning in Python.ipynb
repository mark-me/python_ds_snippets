{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3)\n",
    "model.fit(points)\n",
    "labels = model.predict(new_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = model.cluster_centers_\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters = k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['labels'], df['varieties'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_movements = normalize(movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the labels (results) from the hierarchical clustering, you can apply the fclust function, specifying the 'cutting' height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fcluster(mergings, 6, criterion='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TSNE instance: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(learning_rate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_features = model.fit_transform(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs, ys, c=variety_numbers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats as pearsonr\n",
    "\n",
    "correlation, pvalue = pearsonr(width, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorrelate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA()\n",
    "\n",
    "pca_features = model.fit_transform(grains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot PCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of components chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "pca_features = pca.transform(scaled_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrices for tf-df (word frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply fit_transform to document: csr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_mat = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA to sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an NMF instance: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the articles: nmf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_features = model.transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. The NMF features you obtained earlier are available as nmf_features, while titles is a list of the article titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[3,]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo']\n",
    "\n",
    "# Compute the dot products (cosine similarities)\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array _artists_ whose rows correspond to artists and whose column correspond to users. The entries give the number of times each artist was listened to by each user.\n",
    "\n",
    "In this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, _MaxAbsScaler_, transforms the data so that all users have the same influence on the model, regardless of how many different artists they've listened to. In the next exercise, you'll use the resulting normalized NMF features for recommendation!\n",
    "\n",
    "This data is part of a larger dataset available [here](http://www-etud.iro.umontreal.ca/~bergstrj/audioscrobbler_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so *norm_features* is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list *artist_names*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen']\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
